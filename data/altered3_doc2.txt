The high content review function allows you to easily generate terabytes of master images and metadata in one run, which need to be stored and organized. This means that an appropriate laboratory information management system (LIMS) must be established. Integrate data flow so that at least data can be retrieved and evaluated quickly. After capturing the image and transferring the data, image analysis is required to extract metadata.Combined with pattern recognition algorithms, heat maps can help identify artifacts, such as edge effects, uneven doses, or just exclude blurred images. All panels should be checked for selected positive and negative controls to indicate values ??within this range, and the control controls can be used to normalize the data to identify possible matches without further statistical analysis.It should be evaluated, and there should be good internal control over the accuracy of the analysis and workflow. The results should be verified by restoring to the original image. In addition, the results should be compared between independent runs. Then a strategy for validating reasonable results should be adopted. The expression of the target gene should be confirmed, for example, by microarray analysis of the gene expression of a specific cell line.Compare it with other internal and external data sources. Group analysis helps to identify networks and correlations.
A key aspect of high content screening is the data and information management solutions that users must implement in order to process and store images. Usually, multiple images of different magnifications are collected on a lunar microplate and processed using pre-optimized algorithms (these are software processes that analyze the images). , To identify patterns and extract measurement values ??suitable for biological applications, so you can automatically perform quantitative comparison and classification of compound effects to obtain numerical data of various parameters.Detailed cellular measurements based on the quantification of the observed phenotype. From the perspective of image analysis, the following points should not be overlooked when reviewing the supplierï¿½s recommendations: the breadth of the biological scope; how to deliver the software, quickly start the software or open the script; the analysis is done "on the fly" or offline. Has the algorithm been fully verified by biology? It is easy to export image files to other software packages; and to use new algorithms, is it relatively easy for users to rely on providers, or to develop their own algorithms or adapt existing algorithms?
The main theme and information repeated in this chapter is "association". Research and computer science must work together to facilitate discovery, for example in the drug discovery process. To truly be part of the winning team in the organization, all areas need to combine their collective knowledge and make extra efforts to understand and distinguish each other when the experienced and knowledgeable team members do not understand or seek outside advice. ...You need to first determine the stage of laboratory informatics development, including data management (which we will discuss later in this chapter), in order to have the necessary understanding of where you are now and the direction of development we expect. we. In the future housing industry.
The purpose of this chapter is to outline the key aspects of computing tools and techniques required for housing and community services, including the characteristics of housing and community services data. Data model/structure used to store HCS data; HCSinformatics system architecture, data management methods, hardware and network aspects, imaging, data mining technology, and data integration of HCS with other data and systems. HCS system scans with cells in each hole Or a multi-well plate of cell components, and record multiple cell images. And extract a variety of functions (or measured values) related to biological applications, thereby generating a large amount of data and images.The amount of data and images generated from a single subtitle disk can range from hundreds of megabytes (MB) to several gigabytes (GB). Extensive HCS experiments usually require billions of storage space to produce billions of functions and millions of images. Managing large amounts of HCS data and images requires advanced infrastructure and data processing tools. There are many general rules for computer infrastructure based on HCS images in academic and non-academic organizations.The entire organization analyzed the answers to the following questions, indicating the strategies and configurations the organization should adopt, and what tasks should be allocated to experts and researchers. When deciding on strategy and establishing an organization, you need to answer the following questions:
Is the required analysis software available from the factory or must it be written in-house? This decision must be made in collaboration between IT and scientists based on specific requirements.
What data is collected (how many screens are there per year)?
How to store, manage and protect data for short-term, medium-term and long-term use?
What kind of computing device clusters and servers are required for HCS computing? (brand, type, speed, and memory)
How does the computer system interact with the necessary data collection tools when connecting to the network and the server?
Can scientists make concessions and adjustments to external cooperation and joint plans?
Are we interested in creating a secure buffer outside the firewall to allow this external communication?
After analyzing these issues, you can hire IT professionals from the IT department to work with academics so that IT professionals can take over IT responsibilities. Parallel staff enables IT organizations to understand the requirements of HCS. The server process can be hosted in a pipeline or HCS infrastructure instead of having to be hosted as usual, and additional steps need to be added to the workflow.Managed by the IT department and managed by the HCS department in the organization. Smarter than the IT department